

WICKED's approach is to reply on {\em low dimensional
  approximations} and 
{\em multi-objective
evolutionary algorithms} with decomposition
Recent studies on 
of SE data show we do not
need to reason about all the context variables
(since many are redundant or noisy). When applied to
SBSE, low dimensional approximations can
guide mutation strategies to find
better solutions one to two orders of magnitude
faster than standard evolutionary
optimizers~\cite{krall14,krall14b}.
Secondly, work on MOEA/D~\cite{zhang07:TEC}
has
shown the benefits of dividing  problems into multiple cells, then 
optimizing each cell separately.  Note
that this approach is analogous (and actually
pre-dates) work mentioned above on locality and context.

One way to describe the WICKED system is as a  near-linear time variant on MOEA/D.


~\cite{zhang07:TEC})
WICKED began as an experiment with low-dimensional MOEA/D.r

{\scriptsize
\begin{alltt}
if cplx \(\le\) 1.1:                          Effort  Months   Defect Risk
.. if resl \(\le\) 3.2:
.. .. if pvol \(\le\) 1.1:
.. .. .. if site \(\le\) 1.0:
.. .. .. ..  then: ['__25']  #                 ?      ?        ?      ?
.. .. .. ..  else: ['__20']  #          d      ?      ?        ?      ?
.. .. if rely \(\le\) 1.0:
.. .. .. if pcap \(\le\) 0.9:
.. .. .. .. if ltex \(\le\) 1.0:
.. .. .. .. .. if site \(\le\) 1.0:
.. .. .. .. .. ..  then: ['__3']  #     a     ?        ?      ?       ?
.. .. .. .. .. ..  else: ['__1']  #     c     ?        ?      ?       ?
.. .. .. .. .. else: ['__6']  #        b      ?        ?      ?       ?
.. .. .. .. if ltex \(\le\) 1.0:
.. .. .. .. .. if pmat \(\le\) 3.9:
.. .. .. .. .. ..  then: ['__15']  #         ?        ?      ?       ?
.. .. .. .. .. else: ['__15']  #       e     ?        ?      ?       ?
\end{alltt}}
%cplx resl pvol site reply pcal ltex pmat



This section describes WHERE+CART+ENVY+DELTA.

\subsection{WHERE}
WHERE inputs a set of $N$ examples, each of which is a set of decisions $D$
mapped to a set of objectives  $O$, so $N_i= (D,O)$ (and usually $D>1$ and $O>1$ and 
$O<D$).
WHERE clusters the examples on the decisions and reports the average objective
scores for each objective in each cluster.

WHERE uses a dimensionality reduction heuristic proposed by
Faloutsos and Lin~\cite{Faloutsos1995}. The method 
inputs
$N$
examples $N_1,N_2,..$. Next, WHERE
picks any
point $N_i$ at random. Thirdly, WHERE
finds the point  {\em West}~$\in N$ that is
furthest\footnote{
 For this work, we use the standard Euclidean measure recommended for
instance-based reasoning by Aha et al.~\cite{aha91};
i.e. $\sqrt{\sum_i(x_i-y_i)^2}$ where $x_i,y_i$ 
are values normalized 0..1 for the range min..max.}
from $N_i$.
Finally, WHERE
finds the point {\em East}~$\in N$
that is furthest from {\em West} (and 
$c=\mathit{dist}(\mathit{West},\mathit{East})$).

To recursively cluster the data, WHERE iterates over $N_i \in N$
to find
\mbox{$a=\mathit{dist}(N_i,\mathit{West})$},
\mbox{$b=\mathit{dist}(N_i,\mathit{East})$},
\mbox{$x=(a^2 + c^2 - b^2)/(2c)$}.
This  $x$ value is the projection of $N_i$
on the line  running  {\em East} to {\em West}.  WHERE divides
the examples on the median $x$ value,
then recurses on each half. 
Recursion on
$N$ initial
examples stops when a sub-region
contains less that  $M$ examples (e.g. 
$M=\sqrt{N}$).

Note that this four-step  
process requires only $2N$ distance comparisons
per level of recursion and one call to a sorting routine
to find the median value. 
The total time for WHERE is some linear multiple of the sorting time
used to find the median at each level.
Assuming sorting takes time $O(NlogN)$, then we can say 
that WHERE runs in near linear time
(and not the $O(N^2)$ required for 
other clustering algorithms such as K-Means~\cite{hamerly2010making}).

\subsection{CART}

\subsection{ENVY}

\subsection{DELTA}
