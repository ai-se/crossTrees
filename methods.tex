This study ranks methods using the Scott-Knott
procedure recommended by Mittas \& Angelis in their 2013
IEEE TSE paper~\cite{mittas13}.  This method
sorts a list of $l$ treatments with $ls$ measurements by their median
score. It then
splits $l$ into sub-lists $m,n$ in order to maximize the expected value of
 differences  in the observed performances
before and after divisions. E.g. for lists $l,m,n$ of size $ls,ms,ns$ where $l=m\cup n$:


{\footnotesize
\[E(\Delta)=\frac{ms}{ls}abs(m.\mu - l.\mu)^2 + \frac{ns}{ls}abs(n.\mu - l.\mu)^2\]}
Scott-Knott then applies some statistical hypothesis test $H$ to check
if $m,n$ are significantly different. If so, Scott-Knott then recurses on each division.
For example, consider the following data collected under different treatments {\em rx}:


{\scriptsize \begin{verbatim}
        rx1 = [0.34, 0.49, 0.51, 0.6]
        rx2 = [0.6,  0.7,  0.8,  0.9]
        rx3 = [0.15, 0.25, 0.4,  0.35]
        rx4=  [0.6,  0.7,  0.8,  0.9]
        rx5=  [0.1,  0.2,  0.3,  0.4]
\end{verbatim}}
\noindent
After sorting and division, Scott-Knott declares:
\bi
\item Ranked \#1 is rx5 with median= 0.25
\item Ranked \#1 is rx3 with median= 0.3
\item Ranked \#2 is rx1 with median= 0.5
\item Ranked \#3 is rx2 with median= 0.75
\item Ranked \#3 is rx4 with median= 0.75
\ei
Note that Scott-Knott found  little
difference between rx5 and rx3. Hence,
they have the same rank, even though their medians differ.

Scott-Knott is preferred to, say, hypothesis testing
over all-pairs of methods\footnote{e.g. Six treatments
can be compared $(6^2-6)/2=15$ ways.
A 95\% confidence test run 15 times total confidence 
$0.95^{15} = 46$\%.}.
To avoid an all-pairs comparison, Scott-Knott only calls on hypothesis
tests {\em after} it has found splits that maximize the perfromance differences.
 
For this study, our hypothesis test $H$ was a
conjunction of the A12 effect size test of  and
non-parametric bootstrap sampling; i.e. our
Scott-Knott divided the data if {\em both}
bootstrapping and an effect size test agreed that
the division was statistically significant (99\%
confidence) and not a ``small'' effect ($A12 \ge
0.6$).

For a justification of the use of non-parametric
bootstrapping, see Efron \&
Tibshirani~\cite[p220-223]{efron93}.
For a justification of the use of effect size tests
see Shepperd\&MacDonell~\cite{shepperd12a} and Kampenes~\cite{kampenes07}. These researchers
warn that even if an
hypothesis test declares two populations to be
``significantly'' different, then that result is
misleading if the ``effect size'' is very small\footnote{For
example, Kocaguenli et al.~\cite{kocharm13} report on the misleading
results of such hypothesis tests in software defect
prediction (due to small size of the effect being
explored).}.
Hence, to assess 
the performance differences 
we first must rule out small effects.
Vargha and Delaney's
non-parametric 
A12 effect size test 
explores
two lists $M$ and $N$ of size $m$ and $n$. The counter $\mathit{A12}$ is incremented
$\forall x\in M, y \in N$ as follows:
\bi
\item  If $x > y$ then add $1/(mn)$; 
\item If $x=y$ then add $0.5/(mn)$.
\ei
A12 reports the probability that numbers in one sample 
are bigger than in another.
The A12 thresholds for ``small,medium,large''
effect are  $\{0.56, 0.64,0.71\}$
respectively where ``small'' is a euphemism for trivial or negligible effect.
This test was recently 
endorsed by Arcuri and Briand
at ICSE'11~\cite{arcuri11}.
