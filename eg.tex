
As motivation, this section  offers the real-world goal that
sparked this work.  The Software Engineering
Institute (SEI: http://www.sei.cmu.edu/) at Carnegie
Mellon University is a Federally-Funded Research and
Development Center. As such, SEI works with many
software and software-intensive systems, which are
primarily undertaken for the United States
government (and the US Department of Defense in
particular) but also in commercial industry as
well. As a result of involvement in these programs,
the SEI is a storehouse for multiple repositories of
qualitative and quantitative data collected from
software development.
Developers and managers from across the country look
to SEI for explanations of what factors effect their
project (these explanations are used to manage their
current projects and well as propose methods on how
to better handle their future projects in a better
manner).  Also, SEI researchers may have a
voice in large-scale governmental policy decisions about
information technology.

 
{\em Fact sheets} are one tool used by the SEI for explaining its advice
on best practices and lessons learned
(with
traceability back to the data on which they are
based).  These
explanations are short reports (one to two pages)  which are intended
to give busy managers quick guidance for their
projects. In the case of quantitative data, these
explanations may contain some 2D plot showing how
one objective (e.g. defects) changes in response to
changes in one input variable (e.g. lines of code).

 
Since they are aimed at communicating with busy
professionals on specific points, the fact sheets of
necessity are tradeoffs  between
understandability and accuracy. 
Overly simplistic fact sheets,
while approachable for a larger audience,
can be misleading.  For example, consider  2D plots:
even for single goal
reasoning such as defect reduction, these are
poorly characterized via one input variable. Studies with SE
data have compared models learned  $N =
1$  and $N > 1$ input variables. The models using
more than one input performed better~\cite{me07b}.
Further, many recent SE research publications that propose multiple competing
goals for SE models; e.g.
\bi
\item
Build software {\em faster} using {\em less} effort with {\em fewer} bugs~\cite{elrawas10};
\item
Return defect predictors that find {\em most} defects in the {\em smallest} parts of the 
\cite{arisholm6}.
\ei
As we move from single goal to multiple-goal reasoning, the value of examining important factors in isolation using relatively simple plots
becomes even more questionable. The SBSE experience is that reasoning and
trading off between multiple goals is much more complex than browsing effects related
to a single isolated goal.

 
Additionally, given all the context variables that can be used to describe different
software projects, we recognize it is unlikely that any 
{\em one} report can explain {\em all} the effects
seen in all different kinds of software projects. Several reports in empirical
SE offer the same {\em locality effect}; i.e. models built from  {\em all}  data perform
differently, and often worse, than those learned from specific subsets~\cite{posnet11,betta12,me12d,yang13,emse12}.


How can we augment SEI's fact sheets to  explain:
\bi
\item
The space of effects in multiple dimensions of inputs?
\item
The responses of multiple objectives to changes in those inputs?
\item
And do so across the space of multiple contexts?
\ei
We propose printing succinct decision trees (generated by WICKED)
on the SEI fact sheets. Standard decision trees have leaves
predicting for a single class variable. The leaves of WICKED's trees, on the other hand,
comment on multiple objectives.

For example, \fig{egree} shows a decision tree whose
leaves divide 500 decisions into the eleven groups
``a,b,c,d,e,f,g,h,i,j,k''.  These decisions lead to five
objectives: delivered lines of code, efort,
months, defects, risks (for details on how this
model, see later in this paper).  Some clusters 
dominate  others\footnote{Cluster $i$ dominates $j$ if
no objective  of $j$ scores worse in $j$
and at least one objective is better.} so we say that
those clusters ENVY another (see last column
of \fig{egree}).

When a cluster envies many
other clusters, we select the one with the minimum
DELTA (for cluster $i$ that envies clusters $k\in \{l,m,o...\}$, 
find  the smallest set difference $\Delta$
between the tree branches $b_j,b_i$
such that $j\in k \wedge \Delta=b_i - b_j$). For example,  \fig{egree},
while cluster ``b'' envies three other clusters ``a,d,k'',
we recommend the minimal change that can drive members of ``b''
into cluster ``a'' (just increase programm and language experience, or {\em plex},
over its minimal value of 1.0).

While the ENVY and DELTA process can be fully automated, it is possible
for humans to compute ENVY and DELTA for themselves. That is, if the SEI's
fact sheets including trees like \fig{egree}, business users could 
conduct their own ENVY+DELTA studies as they debate options within their projects.



\begin{figure}[!t]
\input{naveen/sample_tree}
\caption{Sample output from W.I.C.K. (WHERE4, INFOGAIN, CART, KILL) for an SE process model with five objectives:
deliver {\em more} lines of code with {\em fewer} defects in {\em least} effort and time
(effort is total staff hours; time is calendar time start to end of the project).}\label{fig:egree}
\end{figure}

When we show
trees like the above,
one question we are often is
asked in how  such
a simple structure can summarize
the complex trade offs in a 
complex four-objective
SE problem?
