
As motivation, this section  offers the real-world goal that
sparked this work.  
\subsection{The Problem with ``Fact Sheets''}
The Software Engineering
Institute (SEI: http://www.sei.cmu.edu/) at Carnegie
Mellon University is a Federally-Funded Research and
Development Center. As such, SEI works with many
software and software-intensive systems, which are
primarily undertaken for the United States
government (and the US Department of Defense in
particular) but also in commercial industry as
well. As a result of involvement in these programs,
the SEI is a storehouse for multiple repositories of
qualitative and quantitative data collected from
software development.
Developers and managers from across the country look
to SEI for explanations of what factors effect their
project (these explanations are used to manage their
current projects and well as propose methods on how
to better handle their future projects in a better
manner).  Also, SEI researchers may have a
voice in large-scale governmental policy decisions about
information technology.

 
{\em Fact sheets} are one tool used by the SEI for explaining its advice
on best practices and lessons learned
(with
traceability back to the data on which they are
based).  These
explanations are short reports (one to two pages)  which are intended
to give busy managers quick guidance for their
projects. In the case of quantitative data, these
explanations may contain some 2D plot showing how
one objective (e.g. defects) changes in response to
changes in one input variable (e.g. lines of code).

 
Since they are aimed at communicating with busy
professionals on specific points, the fact sheets of
necessity are tradeoffs  between
understandability and accuracy. 
Overly simplistic fact sheets,
while approachable for a larger audience,
can be misleading.  For example, consider  2D plots:
even for single goal
reasoning such as defect reduction, these are
poorly characterized via one input variable. Studies with SE
data have compared models learned  $N =
1$  and $N > 1$ input variables. The models using
more than one input performed better~\cite{me07b}.
Further, many recent SE research publications that propose multiple competing
goals for SE models; e.g.
\bi
\item
Build software {\em faster} using {\em less} effort with {\em fewer} bugs~\cite{elrawas10};
\item
Return defect predictors that find {\em most} defects in the {\em smallest} parts of the 
\cite{arisholm6}.
\ei
As we move from single goal to multiple-goal reasoning, the value of examining important factors in isolation using relatively simple plots
becomes even more questionable. The SBSE experience is that reasoning and
trading off between multiple goals is much more complex than browsing effects related
to a single isolated goal.

 
Additionally, given all the context variables that can be used to describe different
software projects, we recognize it is unlikely that any 
{\em one} report can explain {\em all} the effects
seen in all different kinds of software projects. Several reports in empirical
SE offer the same {\em locality effect}; i.e. models built from  {\em all}  data perform
differently, and often worse, than those learned from specific subsets~\cite{posnet11,betta12,me12d,yang13,emse12}.

\subsection{A Solution}

To address the above, we could add to SEI's fact sheets an explanation of
\bi
\item
The space of effects in multiple dimensions of inputs;
\item
The responses of multiple objectives to changes in those inputs;
\item
And do so across the space of multiple contexts.
\ei
We propose printing succinct decision trees (generated by WICKED)
on the SEI fact sheets. Standard decision trees have leaves
predicting for a single class variable. The leaves of WICKED's trees, on the other hand,
comment on multiple objectives. In this approach, the leaves become different contexts
and the branches of the decision trees becoming context descriptors.

\fig{egree} shows an example of a WICKED decision tree. This tree
divides 500 decisions into the eleven clusters
``a,b,c,d,e,f,g,h,i,j,k''.  These decisions lead to
5 objectives: delivered lines of code, efort,
months, defects, risks (for details on  this
model, see \tion{xomo}).
  
Some clusters
are better than others
so we say that those
clusters ENVY another (see last column
of \fig{egree}).
To be envied, cluster $i$ must dominate $j$; i.e. no objective of $i$ scores ``worse'' in $j$ and at
least one objective is ``better''. Here, ``worse'' and ``better'' refer to the difference
between two populations of objectives in different clusters. To detect that difference, 
we first check for significant difference (using a 95\% confidence bootstrap procedure~\cite{efron93})
and then for a non-small effect (using the A12 test~\cite{Vargha00}). If the difference is significant
and not small, then we compare mean values.

When a cluster envies many other clusters, we focus on those
 with  minimum DELTA. Specifically: for
cluster $i$ that envies clusters
$k\in \{l,m,o...\}$, find the smallest set
difference $\Delta$ between the tree branches
$b_j,b_i$ such that \mbox{$j\in k \wedge \Delta=b_i -
b_j$}.  
For example, in \fig{egree}, while cluster ``b''
envies three other clusters ``a,d,k'', we recommend
the minimal change that can drive members of ``b''
into cluster ``a'' (just increase program and
language experience, or {\em plex}, over its minimal
value of 1.0). 
Note that:
\bi
\item This delta contains decisions present in the dominating cluster,
but absent in the dominated one. This delta is the recommendation generated by WICKED
on how to change the decisions in the dominated cluster.
\item Some clusters
have no recommendation (in \fig{egree}: ``a,c,d,h,k'') since these
clusters envy nothing else. For those clusters, our advice is ``leave well enough alone''.
\item
ENVY and DELTA can be executed manually (just trace out the branches in \fig{egree} using their fingers). That is, if the SEI's fact
sheets including trees like \fig{egree}, business
users could conduct their own ENVY+DELTA  as they
debate options within their projects.
\ei
\begin{figure}[!t]
\input{naveen/sample_tree}
\caption{Right-hand side columns show the mean objective scores for decisions falling into eleven clusters
separated by the left-hand side decision tree. Objective scores are for  five objectives:
deliver {\em more} lines of code with {\em fewer} defects in {\em least} effort and time
(effort is total staff hours; time is calendar time start to end of the project).}\label{fig:egree}
\end{figure}
When we show trees like the above, one question we
are often is asked in how such a simple structure
summarize complex trade offs in a 
multi-objective SE problem? The answer lies in {\em how} the tree is generated.
The recent success of MOEA/D algorithms~\cite{zhang07:TEC} inspired use to try dividing the decision
space, rather than generating one trite set of recommendations for the whole space.
Further, the algorithm that divides the space does not do so using the raw dimensions.
Rather, it operates in a manner analagous to principle component analysis~\cite{pearson1901}, so
WICKED's
divisions occur along  dimensions synthesized in such a way as to (a)~ignore
irreleancies and (b)~combine related terms into a single dimension. Lastly,
WICKED makes extensive use of feature selection~\cite{pearson1901} to remove any surviving decisions
that do not influence the objectives. For details on all those methods, see below.
